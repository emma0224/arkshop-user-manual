# GPT2

GPT2是OPen AI发布的一个预训练语言模型，GPT-2在文本生成上有着惊艳的表现，其生成的文本在上下文连贯性和情感表达上都超过了人们的预期。仅从模型架构而言，GPT-2 并没有特别新颖的架构，GPT-2继续沿用了原来在GPT中使用的单向 Transformer 模型，而这篇文章的目的就是尽可能利用单向Transformer的优势，做一些BERT使用的双向Transformer所做不到的事。那就是通过上文生成下文文本。

参考：[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language\_models\_are\_unsupervised\_multitask\_learners.pdf)
